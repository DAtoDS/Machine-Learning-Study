<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>1. KNN</title><style>
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.highlight-default {
}
.highlight-gray {
	color: rgb(155,154,151);
}
.highlight-brown {
	color: rgb(100,71,58);
}
.highlight-orange {
	color: rgb(217,115,13);
}
.highlight-yellow {
	color: rgb(223,171,1);
}
.highlight-teal {
	color: rgb(15,123,108);
}
.highlight-blue {
	color: rgb(11,110,153);
}
.highlight-purple {
	color: rgb(105,64,165);
}
.highlight-pink {
	color: rgb(173,26,114);
}
.highlight-red {
	color: rgb(224,62,62);
}
.highlight-gray_background {
	background: rgb(235,236,237);
}
.highlight-brown_background {
	background: rgb(233,229,227);
}
.highlight-orange_background {
	background: rgb(250,235,221);
}
.highlight-yellow_background {
	background: rgb(251,243,219);
}
.highlight-teal_background {
	background: rgb(221,237,234);
}
.highlight-blue_background {
	background: rgb(221,235,241);
}
.highlight-purple_background {
	background: rgb(234,228,242);
}
.highlight-pink_background {
	background: rgb(244,223,235);
}
.highlight-red_background {
	background: rgb(251,228,228);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(55, 53, 47, 0.6);
	fill: rgba(55, 53, 47, 0.6);
}
.block-color-brown {
	color: rgb(100,71,58);
	fill: rgb(100,71,58);
}
.block-color-orange {
	color: rgb(217,115,13);
	fill: rgb(217,115,13);
}
.block-color-yellow {
	color: rgb(223,171,1);
	fill: rgb(223,171,1);
}
.block-color-teal {
	color: rgb(15,123,108);
	fill: rgb(15,123,108);
}
.block-color-blue {
	color: rgb(11,110,153);
	fill: rgb(11,110,153);
}
.block-color-purple {
	color: rgb(105,64,165);
	fill: rgb(105,64,165);
}
.block-color-pink {
	color: rgb(173,26,114);
	fill: rgb(173,26,114);
}
.block-color-red {
	color: rgb(224,62,62);
	fill: rgb(224,62,62);
}
.block-color-gray_background {
	background: rgb(235,236,237);
}
.block-color-brown_background {
	background: rgb(233,229,227);
}
.block-color-orange_background {
	background: rgb(250,235,221);
}
.block-color-yellow_background {
	background: rgb(251,243,219);
}
.block-color-teal_background {
	background: rgb(221,237,234);
}
.block-color-blue_background {
	background: rgb(221,235,241);
}
.block-color-purple_background {
	background: rgb(234,228,242);
}
.block-color-pink_background {
	background: rgb(244,223,235);
}
.block-color-red_background {
	background: rgb(251,228,228);
}
.select-value-color-default { background-color: rgba(206,205,202,0.5); }
.select-value-color-gray { background-color: rgba(155,154,151, 0.4); }
.select-value-color-brown { background-color: rgba(140,46,0,0.2); }
.select-value-color-orange { background-color: rgba(245,93,0,0.2); }
.select-value-color-yellow { background-color: rgba(233,168,0,0.2); }
.select-value-color-green { background-color: rgba(0,135,107,0.2); }
.select-value-color-blue { background-color: rgba(0,120,223,0.2); }
.select-value-color-purple { background-color: rgba(103,36,222,0.2); }
.select-value-color-pink { background-color: rgba(221,0,129,0.2); }
.select-value-color-red { background-color: rgba(255,0,26,0.2); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="4a676434-0a09-4d43-b7db-6d929408a5c9" class="page sans"><header><h1 class="page-title">1. KNN</h1></header><div class="page-body"><h3 id="a52de9ef-a468-467f-9e54-656351b4e6ac" class="">원리</h3><ul id="e02e643e-1660-401b-a5bd-d2f6ec2ba86d" class="bulleted-list"><li>서로 가까운 점들은 유사하다는 가정을 가지는 알고리즘</li></ul><ul id="d0bf5c45-fd2e-4e70-a316-ddfa8557f6ed" class="bulleted-list"><li>KNN 알고리즘은 훈련 데이터셋에서 새로운 데이터 포인트와 가장 가까운 훈련 데이터 포인트(최근접 이웃)를 찾아 이를 통해 예측을 진행<figure id="c25ef1e4-d80b-492f-800b-1e630eb731e6" class="image"><a href="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled.png"><img style="width:405px" src="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled.png"/></a></figure></li></ul><h3 id="f5085b7c-323a-4627-b385-2d46013f0e82" class="">종류</h3><ul id="2b082a82-f4f0-4a62-80eb-3a1f809a455c" class="bulleted-list"><li>K-최근접 이웃 분류 <ul id="d0acc390-2473-4e7f-a547-bf48badaa46f" class="bulleted-list"><li>새로운 데이터 포인트와 가까운 훈련 데이터 포인트 K개 중 가장 빈도가 높은 것으로 새로운 데이터 포인트를 분류함<ul id="31e8b33d-48ad-4a3e-bc9d-5a2dcbc4a82a" class="bulleted-list"><li>단, 공동 1등이 생긴다면 3가지 조치를 취할 수 있음<ul id="366b53ab-c824-4a14-85db-6b1e3ad4d2f7" class="bulleted-list"><li>여러 1등 중 임의로 하나를 정함</li></ul><ul id="b93b1aeb-52aa-47ad-bd43-7b7cc6a957ea" class="bulleted-list"><li>거리를 가중치로 사용해서 거리 기반 투표를 함</li></ul><ul id="4c284389-4a17-421e-bd51-49a0fb3555f9" class="bulleted-list"><li>단독 1등이 생길 때 까지 K를 하나씩 줄임</li></ul></li></ul></li></ul><figure id="aebf9583-4151-4cc4-9925-fd2a57322a8a" class="image"><a href="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%201.png"><img style="width:1051px" src="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%201.png"/></a></figure></li></ul><ul id="2dfe2ce1-5810-468b-bc34-ffc5d9e8bcd1" class="bulleted-list"><li>K-최근접 이웃 회귀<ul id="2c107a5a-52f0-404d-b3dd-3a811250ea1b" class="bulleted-list"><li>새로운 데이터 포인트와 가까운 훈련 데이터 포인트 K개의 평균으로 새로운 데이터 포인트 값을 예측함<figure id="92b3bfb3-0607-4c36-871e-5362ef8a8ce6" class="image"><a href="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%202.png"><img style="width:1062px" src="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%202.png"/></a></figure></li></ul></li></ul><h3 id="8ea30475-798b-4cda-88bb-81973206f4c0" class="">매개변수</h3><ul id="5de8f9bb-3ac7-4db0-a181-274ea5ee7bc7" class="bulleted-list"><li>데이터 포인트 사이의 거리를 재는 방법(= 유사도 측정 방법)<ul id="f66235b9-4fe5-413e-987f-858ae3a767cf" class="bulleted-list"><li>유클리디언 거리(Euclidean distance)<ul id="8f85fe1b-1609-4db9-be21-ebc2ebe9c4c4" class="bulleted-list"><li>각 속성들 간의 차이를 모두 고려하여 계산</li></ul><ul id="7fbc1765-94d6-4a57-951f-ae923251ae0e" class="bulleted-list"><li>계산값이 0에 가까울수록 유사한 것</li></ul></li></ul><figure id="118f1b15-afde-424e-9c2b-1d36f2b3bf4f" class="image"><a href="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%203.png"><img style="width:480px" src="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%203.png"/></a></figure><ul id="6e31938d-e32c-4be8-b44a-1572841c1304" class="bulleted-list"><li>마할라노비스 거리(Mahalanobis distance)<ul id="b7040f22-5476-4ca0-b813-95a18cd18132" class="bulleted-list"><li>유클리디언 거리에서 데이터의 속성들의 공분산(covariance)을 반영하여 거리를 계산하는 방법</li></ul><ul id="d1e9a0e1-3a69-4116-b134-05fab787e1f2" class="bulleted-list"><li>계산값이 0에 가까울수록 유사한 것</li></ul></li></ul><figure id="c543dcc6-7b98-4baa-98b1-5fe081455b6d" class="image"><a href="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%204.png"><img style="width:400px" src="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%204.png"/></a></figure><ul id="c5e3306f-266c-4733-a64f-d0e73f0bdd60" class="bulleted-list"><li>민코스키 거리(Minkowski distance)<ul id="595ad580-4b4c-4d71-aa83-f660ab31b8ae" class="bulleted-list"><li>유클리디언 거리가 각 속성들 간의 차이를 모두 고려한 거리라면, 민코스키 거리는 가장 큰 차이만을 가지고 거리를 계산하는 방법</li></ul><ul id="c257335b-7f0b-4386-be3c-505ff624b71e" class="bulleted-list"><li>계산값이 0에 가까울수록 유사한 것</li></ul></li></ul><figure id="43ab2ab8-eb75-4ebc-8526-346a95f71ca3" class="image"><a href="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%205.png"><img style="width:400px" src="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%205.png"/></a></figure><ul id="ce9f1bc0-d5c7-481e-8067-ce28565f033e" class="bulleted-list"><li>코사인 유사도(Cosine simliarity)<ul id="84a4c395-861b-4585-8d13-dee6d48cd76e" class="bulleted-list"><li>각(radian) 기반의 계산법</li></ul><ul id="973d9ae0-e0cd-47a1-8252-26816014615b" class="bulleted-list"><li>벡터의 크기에 영향을 받지 않는 특징</li></ul><ul id="fdefe5cb-289d-4fb3-9218-c20166584573" class="bulleted-list"><li>값의 범위는 -1~1이며, 1에 가까울수록 유사한 것</li></ul></li></ul><figure id="83e5d46c-2e84-462d-90f8-b42ef77109f0" class="image"><a href="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%206.png"><img style="width:400px" src="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%206.png"/></a></figure></li></ul><ul id="c37470d2-8af9-4a75-b9b5-29dc3afca837" class="bulleted-list"><li>이웃의 수(K)<ul id="532f9758-a45e-43c8-a34a-eda0e852cd25" class="bulleted-list"><li>3개나 5개 정도로 적을 때 잘 작동함</li></ul><ul id="65d64cd1-9a38-4b60-be47-a16e8010bd88" class="bulleted-list"><li>이웃의 수가 커지면 결정 경계는 부드러워지고(단순한 모델), 이웃의 수가 작아지면 결정경계는 훈련 데이터에 가깝게 됨(복잡한 모델)<figure id="ff2014b7-395e-4d48-931c-0e44523dda14" class="image"><a href="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%207.png"><img style="width:1069px" src="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%207.png"/></a></figure><figure id="f5c4c2ab-29a2-40ce-a37d-f10e5e5d9cbc" class="image"><a href="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%208.png"><img style="width:1053px" src="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%208.png"/></a></figure></li></ul><ul id="7ee0c1b8-f256-4238-9258-91eabeda75b7" class="bulleted-list"><li>훈련 데이터 전체 개수를 이웃의 수로 지정하는 극단적인 경우, 모든 테스트 포인트가 같은 이웃을 가지게 되므로 테스트 포인트에 대한 예측값은 모두 같은 값이 됨(=즉, 훈련 세트에서 가장 많은 데이터 포인트를 가진 클래스가 예측값이 됨). 그래서 k가 너무 작아도 안되고 너무 커도 문제가 있기 때문에 적당한 K의 개수를 구할 필요가 있음<figure id="84133b53-713b-4ab3-84d3-2f4c3cfacdd9" class="image"><a href="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%209.png"><img style="width:1033px" src="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%209.png"/></a></figure></li></ul></li></ul><h3 id="1b8cedf2-f503-4fa0-9983-cb9487bfadaf" class="">장점</h3><ul id="19c244b1-f31b-467c-911c-a63d8da4fc0f" class="bulleted-list"><li>이해하기 쉬운 모델</li></ul><ul id="874c4182-0b9c-41df-aada-1ecf13313fea" class="bulleted-list"><li>매개변수를 많이 조정하지 않아도 자주 좋은 성능을 발휘함</li></ul><ul id="56e15081-6f9d-4136-859f-961950bf2e43" class="bulleted-list"><li>더 복잡한 알고리즘을 적용해보기 전에 시도해볼 수 있음(=베이스 라인)</li></ul><ul id="5bff983d-131a-440b-acb3-bcce1bbfaab4" class="bulleted-list"><li>모델 훈련 시간이 필요 없어서 매우 빠르게 모델 구축이 가능함</li></ul><h3 id="6b87a3e6-49c1-46ef-8bb7-89e9243546b1" class="">단점</h3><ul id="508d57e7-2a6a-43b8-aad3-5c1671b36639" class="bulleted-list"><li>데이터 전처리 과정이 매우 중요<ul id="ed2e4840-e176-425f-87d8-3b3df6cda93c" class="bulleted-list"><li>수치형 데이터들의 값을 같은 범위로 맞춰주어야 함<ul id="36e5e80b-190a-4bda-94b7-875dc809dfb9" class="bulleted-list"><li>정규화(normalization)<pre id="7182ff11-3f6e-406e-ab85-20ab4ac6087c" class="code"><code>normalized = (x-min(x))/(max(x)-min(x))</code></pre></li></ul><ul id="1d776ccd-3c36-4d0f-80b2-1921369eec63" class="bulleted-list"><li>표준화(standardization)<pre id="a1bdf64c-ce33-4fc7-b1d1-66ca1970a2a7" class="code"><code>standardized = (x-mean(x))/std(x)</code></pre></li></ul></li></ul><ul id="3489515f-1419-4776-92c7-4e0de62b5ec1" class="bulleted-list"><li>명목/범주형 데이터의 경우 one hot encoding을 사용해 더미 변수로 만들어줘야 함<figure id="e9b3053b-56dd-4986-9418-ee154d1b3fde" class="image"><a href="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%2010.png"><img style="width:700px" src="1%20KNN%20c25ef1e4d80b492f800b1e630eb731e6/Untitled%2010.png"/></a></figure></li></ul></li></ul><ul id="e4ccce5a-42f5-46da-866a-17008e47734b" class="bulleted-list"><li>훈련 세트(특성의 수, 샘플의 수)가 너무 크면 예측이 느려짐</li></ul><ul id="14fa9cf8-df97-434e-98cb-d1a0298174b0" class="bulleted-list"><li>수백 개 이상의 많은 특성을 가진 데이터셋에는 잘 동작하지 않음<ul id="e067403d-0370-422d-9faa-b3792e96131f" class="bulleted-list"><li>차원의 저주<ul id="f69ac777-da4c-4e5a-b1d0-be6d5ac3c3c9" class="bulleted-list"><li>두 점이 가깝다라고 하려면 모든 차원에 대해 가까워야 하기 때문에, 차원이 추가된다는 것은 두 점이 가까울 수 있는 가능성이 현저히 줄어든다는 것을 의미함</li></ul><ul id="2561cd39-c8e6-4043-a40f-f545f0281cfe" class="bulleted-list"><li>고차원일 때는 근접이웃들이 평균 거리와 큰 차이가 나지 않게 되고, 그렇기 때문에 가깝다는 것이 별 의미를 가지지 않게 됨</li></ul><ul id="04186537-b955-4e88-bc6b-d696192fc4ff" class="bulleted-list"><li>따라서, 고차원에서 KNN을 이용하려면 먼저 차원 축소를 하는 것이 좋음</li></ul></li></ul></li></ul><ul id="a7a869aa-bf1b-4196-8a7a-8b7d342419ad" class="bulleted-list"><li>특성 값이 대부분 0인, 희소한 데이터셋과는 특히 잘 작동하지 않음</li></ul><p id="f592c5c7-b19c-4b30-bf8f-b880936db478" class="">
</p><h3 id="5fb587ea-48af-4b7e-83cf-1a240c8a224f" class="">참고</h3><ul id="411cd8f2-5da7-4781-aee0-26abfaf4695e" class="bulleted-list"><li>[블로그]유사도 측정 방법<ul id="698a94a0-0c3e-4d7a-8767-3cc76356789a" class="bulleted-list"><li><a href="https://m.blog.naver.com/PostView.nhn?blogId=cjh226&amp;logNo=220810613028&amp;proxyReferer=https:%2F%2Fwww.google.com%2F">https://m.blog.naver.com/PostView.nhn?blogId=cjh226&amp;logNo=220810613028&amp;proxyReferer=https:%2F%2Fwww.google.com%2F</a></li></ul></li></ul><ul id="7af69287-0e9f-4785-829e-e9847db464e5" class="bulleted-list"><li>[책]파이썬 라이브러리를 활용한 머신러닝</li></ul><ul id="6fb46302-5ac1-4fc7-9529-aa75d2519861" class="bulleted-list"><li>[책]밑바다부터 시작하는 데이터 과학</li></ul><p id="eb126bcd-c250-4bc3-8a77-f6ceafd62b8e" class="">
</p></div></article></body></html>